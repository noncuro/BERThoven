# BERThoven

In this repository, you will find the implementation of our different models that predicts the quality of translation between English and German. 
The dataset used to train these models constructed this way:
- src : Source document
- mt  : Machine translation
- scores : z-standardised score of three anotator's raw scores

The model outputs for each tupe (src, mt) a z-score

---

## Getting started
These instructions will get you a copy of the project up and running on your local machine

### Prerequisites
The requirements to run the project are the following
+ python 3.6
+ numpy
+ matplotlib
+ torch
+ tqdm
+ transformers
+ sklearn
+ scipy
+ pandas
+ requests
+ ipython

Install requirement:
```bash
pip3 install -r ./requirements.txt
```

---
## Installation and usage

### BERT model
You will find a notebook containing sample code to run our models in `Sample Notebook using BERT.ipynb`

### BI-LSTM model
We did not choose the BI-LSTM model as our final model. Therefore it is not present in the Notebook. However the next steps show how to run it locally. 

#### Get the files
To get the files:
```bash
wget https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d -O files.zip
unzip files.zip
```
#### Run the model
You will find a sample code to run the training of our BI-LSTM model in `train_bilstm.py`

---

### Parameter search
Run the lines bellow. First line generate the experiments, second line run the experiments generated by first script

```bash
python3 ./param_search/generate_experiments.py
python3 ./param_search/run_experiments.py experiments_set_1.json
```
---


## Documentation

### bert_lib
Contains code related to our BERT-based model.

- #### BERT_utils.py
  `get_new_bert_model`: Returns a new BERT model with guaranteed freshly loaded weights. Avoids retraining models on top of others if you're not careful

  `is_model_new`: Verifies if the BERT model has fresh weights, i.e. hasn't been trained yet.

  `BERTHovenDataset`: Class responsible of handling pre-processing of data handed to BERT, for use within a dataloader

  `MaskedDataset`: Class responsible of handling pre-processing of data handed to BERT. This modification allows for words in the sentence to randomly be replaced by [MASK] tokens. This can be seen as a way to introduce noise into the system, to reduce the likelihood of overfitting.

  `get_tokenized`: Performs tokenization and index substitution on a dataframe. This function also concatenates the sentences in both ways, appending a [SEP] token after each one.

  `get_data_loader`: Returns a Torch DataLoader object containing the dataset

  `get_data_loader_masked`: Returns a Torch DataLoader object containing the dataset. This DataLoader applies random masking to its data

  `get_sentence_embeddings`: Returns the embeddings of a sentence as the pooled outputs from a pretrained BERT model.


- #### BERThoven_model.py
  `BERThoven` : Pytorch model based on pretrained BERT
  
- #### train_BERT.py
  `check_accuracy`: Check the accuracy over a validation dataset. This is used during training to check if the model is overfitting.

  `train_part`: Train a BERTHoven model

  `get_test_labels`: Returns the score given by a trained model on a test set.

  `writeScores`: Write the scores to a file


### bilstm
Contains code related to our Bi-LSTM attention model. Further comments can be found inside files

- #### bilstm_experiment.py
  Trains and evaluates the model

- #### comparer_model.py
  Defines the encoder and decoder architecture

- #### train_bilstm.py
  Defines the trainer class used in `bilstm_experiment.py`

### param_search
Contains code related to automated hyper parameters search.
- #### generate_experiments.py
  Generates all combinations of hyper parameter values we found interesting

- #### run_experiments.py
  Train models based on generated hyper parameters combinations, and records there perfomance in a JSON file.
