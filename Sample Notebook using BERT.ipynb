{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sample Notebook using BERT.ipynb","provenance":[{"file_id":"1L0_e7LWhX0OGlfAP8a7irUE3TVIq06pJ","timestamp":1582903573056}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"TPswTE8BaNmT","colab_type":"text"},"source":["This notebook will run through an example, training a model for the translation task by fine-tuning BERT. To start, connect to the runtime and upload `bert_lib.zip` and `utils.py`\n","\n","\n","Find this on colab at https://colab.research.google.com/drive/16I9u8s92EYmqiPkevzhUycCiE5WhGBQo"]},{"cell_type":"code","metadata":{"id":"T7wcqPQtmXIL","colab_type":"code","colab":{}},"source":["# Run this only once\n","\n","!wget https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d -O files.zip\n","!unzip files.zip\n","!pip install transformers\n","!unzip bert_lib.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QfUlxtrvmgT-","colab_type":"code","colab":{}},"source":["from bert_lib import * \n","from tqdm import tqdm_notebook as tqdm\n","import transformers\n","from transformers import (AdamW,\n","                          get_linear_schedule_with_warmup,get_constant_schedule_with_warmup)\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","import scipy\n","import time\n","import sklearn\n","from sklearn import preprocessing, pipeline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uqb_sPmW2_YY","colab_type":"code","colab":{}},"source":["USE_GPU = True\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    raise RuntimeError(\"We reaaaaaally recommend you use a GPU...\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PnhFsNB4ec9n","colab_type":"code","colab":{}},"source":["train_df, dev_df = import_train_dev(1/8)\n","test_df = import_file(\"test\")\n","\n","score_preprocessor = sklearn.pipeline.make_pipeline(\n","    # More preprocessing steps, to be applied to scores, can go here.\n","    preprocessing.MinMaxScaler()\n",")\n","\n","# We start with a data loader that runs with just the original dataset\n","data_loader_train = get_data_loader(train_df, batch_size=32, preprocessor=score_preprocessor, fit=True)\n","data_loader_dev = get_data_loader(dev_df, batch_size=32, preprocessor=score_preprocessor)\n","data_loader_test = get_data_loader(test_df, batch_size=32, test=True, preprocessor=score_preprocessor)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N95QiSaHJ_qY","colab_type":"code","colab":{}},"source":["train_df_without_sames = train_df[train_df.src != train_df.mt]\n","q1 = train_df.scores.quantile(0.25)\n","q3 = train_df.scores.quantile(0.75)\n","\n","# Create a dataloader that upsamples outliers\n","train_df_upsampling = augment_dataset(train_df_without_sames, \n","                            lambda score: score<q1, \n","                            lambda score: score>q3)\n","data_loader_train_upsampled = get_data_loader(train_df_upsampling, batch_size=32,preprocessor=score_preprocessor)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Po9zrBTQcW9P","colab_type":"code","colab":{}},"source":["\n","# A dataloader that removes outliers:\n","# iqr = q3-q1\n","# train_df_downsampling = train_df_without_sames[train_df_without_sames.scores>q1-3*iqr]\n","# data_loader_train_downsampled = get_data_loader(train_df_downsampling, batch_size=32,preprocessor=score_preprocessor)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZJqSkkg8cXo7","colab_type":"code","colab":{}},"source":["# A dataloader that with added masks:\n","\n","data_loader_train_masked = get_data_loader_masked(train_df_without_sames, batch_size=32, preprocessor=score_preprocessor,)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q6i9tDBmyGYJ","colab_type":"code","colab":{}},"source":["model = BERThoven(cls=True, dropout=False, concat_outputs=False)\n","check_accuracy(data_loader_dev, model, device=device, preprocessor=score_preprocessor);\n","# Get a baseline for the model's metrics before any training\n","\n","warmup_proportion = 0.1\n","\n","delta = 0.1 # Smooth l1 switches between L1 and MSE at delta\n","loss_function = lambda x, y: F.smooth_l1_loss(x/delta, y/delta)*delta\n","\n","training_steps = 0\n","training_steps += len(data_loader_train_upsampled)\n","training_steps += len(data_loader_train_masked)\n","training_steps += len(data_loader_train)\n","\n","warmup_steps = int(training_steps*warmup_proportion)\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8, correct_bias=True, weight_decay=0.1)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=training_steps)\n","\n","def train(data_loader, epochs):\n","  return train_part(model, data_loader, optimizer, scheduler,  val_loader=data_loader_dev, epochs=epochs, \n","             val_every = 3,return_losses=True, preprocessor=score_preprocessor,\n","             print_every=print_every, loss_function=loss_function, return_metrics=False,device=device)\n","\n","# Train for 1 epoch on each data loader.\n","\n","train(data_loader_train_upsampled, epochs=1);\n","train(data_loader_train_masked, epochs=1);\n","train(data_loader_train, epochs=1);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QM2bQidlkmgP","colab_type":"code","colab":{}},"source":["# Get the test labels\n","scores = get_test_labels(data_loader_test,nlp_model,device,preprocessor=score_preprocessor)\n","\n","# Save them to a file\n","writeScores(scores)"],"execution_count":0,"outputs":[]}]}